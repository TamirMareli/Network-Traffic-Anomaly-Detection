{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d5c9ac",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 1. Imports & Configuration\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c41b6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle # To save the scalers/encoders for later use\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8fa407",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 2. Load Cleaned Data\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca78a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned data...\n",
      "Train shape: (125973, 42)\n",
      "Test shape: (22544, 42)\n"
     ]
    }
   ],
   "source": [
    "input_dir = '../data/processed'\n",
    "output_dir = '../data/processed' # We will save final arrays here\n",
    "\n",
    "print(\"Loading cleaned data...\")\n",
    "train_df = pd.read_csv(os.path.join(input_dir, 'train_cleaned.csv'))\n",
    "test_df = pd.read_csv(os.path.join(input_dir, 'test_cleaned.csv'))\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2511538",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 3. Label Mapping (Attack Classes)\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "937b4ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train attack class distribution:\n",
      "attack_class\n",
      "Normal    67343\n",
      "DoS       45927\n",
      "Probe     11656\n",
      "R2L         995\n",
      "U2R          52\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test attack class distribution (including Unknown):\n",
      "attack_class\n",
      "Normal     9711\n",
      "DoS        7458\n",
      "R2L        2885\n",
      "Probe      2421\n",
      "U2R          67\n",
      "Unknown       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# NSL-KDD has specific attack types (e.g., 'neptune', 'satan') that belong to 4 broad categories.\n",
    "# We map them to: Normal, DoS, Probe, R2L, U2R.\n",
    "\n",
    "# Mapping dictionary based on NSL-KDD documentation\n",
    "attack_mapping = {\n",
    "    'normal': 'Normal',\n",
    "    \n",
    "    # DoS (Denial of Service)\n",
    "    'back': 'DoS', 'land': 'DoS', 'neptune': 'DoS', 'pod': 'DoS', 'smurf': 'DoS',\n",
    "    'teardrop': 'DoS', 'mailbomb': 'DoS', 'apache2': 'DoS', 'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "    # Probe (Surveillance and other probing)\n",
    "    'satan': 'Probe', 'ipsweep': 'Probe', 'nmap': 'Probe', 'portsweep': 'Probe',\n",
    "    'mscan': 'Probe', 'saint': 'Probe',\n",
    "    \n",
    "    # R2L (Remote to Local)\n",
    "    'guess_passwd': 'R2L', 'ftp_write': 'R2L', 'imap': 'R2L', 'phf': 'R2L',\n",
    "    'multihop': 'R2L', 'warezmaster': 'R2L', 'warezclient': 'R2L', 'spy': 'R2L',\n",
    "    'xlock': 'R2L', 'xsnoop': 'R2L', 'snmpguess': 'R2L', 'snmpgetattack': 'R2L',\n",
    "    'httptunnel': 'R2L', 'sendmail': 'R2L', 'named': 'R2L',\n",
    "    \n",
    "    # U2R (User to Root)\n",
    "    'buffer_overflow': 'U2R', 'loadmodule': 'U2R', 'perl': 'U2R', 'rootkit': 'U2R',\n",
    "    'ps': 'U2R', 'sqlattack': 'U2R', 'xterm': 'U2R'\n",
    "}\n",
    "\n",
    "# Map labels to high-level attack categories\n",
    "train_df['attack_class'] = train_df['label'].map(attack_mapping).fillna('Unknown')\n",
    "test_df['attack_class']  = test_df['label'].map(attack_mapping).fillna('Unknown')\n",
    "\n",
    "print(\"Train attack class distribution:\")\n",
    "print(train_df['attack_class'].value_counts())\n",
    "\n",
    "print(\"\\nTest attack class distribution (including Unknown):\")\n",
    "print(test_df['attack_class'].value_counts())\n",
    "\n",
    "# Binary target for anomaly detection\n",
    "# Normal = 0, Attack / Unknown = 1\n",
    "train_df['binary_target'] = (train_df['attack_class'] != 'Normal').astype(int)\n",
    "test_df['binary_target']  = (test_df['attack_class'] != 'Normal').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117a2174",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 4. Feature Selection: Numeric vs Categorical\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a627041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2] Features defined.\n",
      "Numerical features: 38\n",
      "Categorical features: 3\n"
     ]
    }
   ],
   "source": [
    "# We separate features to treat them differently.\n",
    "# 'label', 'attack_class', 'binary_target' are targets, not features.\n",
    "\n",
    "target_cols = ['label', 'attack_class', 'binary_target']\n",
    "features = [c for c in train_df.columns if c not in target_cols]\n",
    "\n",
    "# Identify categorical columns (strings) and numeric columns\n",
    "categorical_cols = ['protocol_type', 'service', 'flag']\n",
    "numeric_cols = [c for c in features if c not in categorical_cols]\n",
    "\n",
    "print(f\"\\n[2] Features defined.\")\n",
    "print(f\"Numerical features: {len(numeric_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118bf721",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 5. Preprocessing Pipeline (Encoding & Scaling)\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae1a1f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3] Building Preprocessing Pipeline...\n",
      "Fitting preprocessor on Train set and transforming...\n",
      "New X_train shape: (125973, 122)\n",
      "New X_test shape: (22544, 122)\n",
      "Total features names extracted: 122\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3] Building Preprocessing Pipeline...\")\n",
    "\n",
    "# Define the transformer\n",
    "# 1. StandardScaler for numeric features (mean=0, std=1)\n",
    "# 2. OneHotEncoder for categorical features (handle_unknown='ignore' is CRITICAL for test set)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split X (features) and y (targets)\n",
    "X_train_raw = train_df[features]\n",
    "y_train_multi = train_df['attack_class']\n",
    "y_train_binary = train_df['binary_target']\n",
    "\n",
    "X_test_raw = test_df[features]\n",
    "y_test_multi = test_df['attack_class']\n",
    "y_test_binary = test_df['binary_target']\n",
    "\n",
    "# Fit on TRAIN, Transform on BOTH\n",
    "# This ensures we don't \"peek\" at the test data statistics (Data Leakage prevention)\n",
    "print(\"Fitting preprocessor on Train set and transforming...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train_raw)\n",
    "X_test_processed = preprocessor.transform(X_test_raw)\n",
    "\n",
    "print(f\"New X_train shape: {X_train_processed.shape}\")\n",
    "print(f\"New X_test shape: {X_test_processed.shape}\")\n",
    "\n",
    "# Get feature names after One-Hot Encoding (for interpretability later)\n",
    "# Note: This syntax depends on sklearn version. If error, wrap in try-except.\n",
    "try:\n",
    "    num_names = numeric_cols\n",
    "    cat_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "    feature_names = np.r_[num_names, cat_names]\n",
    "    print(f\"Total features names extracted: {len(feature_names)}\")\n",
    "except:\n",
    "    print(\"Could not extract feature names directly (sklearn version difference).\")\n",
    "    feature_names = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcde826",
   "metadata": {},
   "source": [
    "# ==========================================\n",
    "# 6. Save Processed Data for Modeling\n",
    "# =========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a837db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4] Saving processed arrays and objects...\n",
      "Done! Preprocessing complete. Ready for Model Training (03_model_training).\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[4] Saving processed arrays and objects...\")\n",
    "\n",
    "# Save numpy arrays (efficient for ML models)\n",
    "np.save(os.path.join(output_dir, 'X_train.npy'), X_train_processed)\n",
    "np.save(os.path.join(output_dir, 'X_test.npy'), X_test_processed)\n",
    "np.save(os.path.join(output_dir, 'y_train_binary.npy'), y_train_binary)\n",
    "np.save(os.path.join(output_dir, 'y_test_binary.npy'), y_test_binary)\n",
    "np.save(os.path.join(output_dir, 'y_train_multi.npy'), y_train_multi)\n",
    "np.save(os.path.join(output_dir, 'y_test_multi.npy'), y_test_multi)\n",
    "\n",
    "# Save feature names and the preprocessor object (optional, good for inference)\n",
    "if feature_names is not None:\n",
    "    pd.DataFrame({'feature': feature_names}).to_csv(os.path.join(output_dir, 'feature_names.csv'), index=False)\n",
    "\n",
    "# Saving the preprocessor allows us to inverse_transform or process single inputs later\n",
    "with open(os.path.join(output_dir, 'preprocessor.pkl'), 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "\n",
    "print(\"Done! Preprocessing complete. Ready for Model Training (03_model_training).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
